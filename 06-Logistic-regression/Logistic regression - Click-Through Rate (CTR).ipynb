{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML Logo](http://spark-mooc.github.io/web-assets/images/CS190.1x_Banner_300.png)\n",
    "# **Click-Through Rate Prediction Lab**\n",
    "#### This lab covers the steps for creating a click-through rate (CTR) prediction pipeline.  You will work with the [Criteo Labs](http://labs.criteo.com/) dataset that was used for a recent [Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge).\n",
    "#### ** This lab will cover: **\n",
    "+  ####*Part 1:* Featurize categorical data using one-hot-encoding (OHE)\n",
    "+  ####*Part 2:* Construct an OHE dictionary\n",
    "+  ####*Part 3:* Parse CTR data and generate OHE features\n",
    " + #### *Visualization 1:* Feature frequency\n",
    "+  ####*Part 4:* CTR prediction and logloss evaluation\n",
    " + #### *Visualization 2:* ROC curve\n",
    "+  ####*Part 5:* Reduce feature dimension via feature hashing\n",
    " + #### *Visualization 3:* Hyperparameter heat map\n",
    " \n",
    "#### Note that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labVersion = 'cs190_week4_v_1_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Featurize categorical data using one-hot-encoding **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) One-hot-encoding **\n",
    "#### We would like to develop code to convert categorical features to numerical ones, and to build intuition, we will work with a sample unlabeled dataset with three data points, with each data point representing an animal. The first feature indicates the type of animal (bear, cat, mouse); the second feature describes the animal's color (black, tabby); and the third (optional) feature describes what the animal eats (mouse, salmon).\n",
    "#### In a one-hot-encoding (OHE) scheme, we want to represent each tuple of `(featureID, category)` via its own binary feature.  We can do this in Python by creating a dictionary that maps each tuple to a distinct integer, where the integer corresponds to a binary feature. To start, manually enter the entries in the OHE dictionary associated with the sample dataset by mapping the tuples to consecutive integers starting from zero,  ordering the tuples first by featureID and next by category.\n",
    "#### Later in this lab, we'll use OHE dictionaries to transform data points into compact lists of features that can be used in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data for manual OHE\n",
    "# Note: the first data point does not include any value for the optional third feature\n",
    "sampleOne = [(0, 'mouse'), (1, 'black')]\n",
    "sampleTwo = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sampleThree =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "sampleDataRDD = sc.parallelize([sampleOne, sampleTwo, sampleThree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "sampleOHEDictManual = {}\n",
    "sampleOHEDictManual[(0,'bear')] = 0 #<FILL IN>\n",
    "sampleOHEDictManual[(0,'cat')] = 1 # <FILL IN>\n",
    "sampleOHEDictManual[(0,'mouse')] = 2 # <FILL IN>\n",
    "sampleOHEDictManual[(1,'black')] = 3\n",
    "sampleOHEDictManual[(1,'tabby')] = 4\n",
    "sampleOHEDictManual[(2,'mouse')] = 5\n",
    "sampleOHEDictManual[(2,'salmon')] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST One-hot-encoding (1a)\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(0,'bear')],\n",
    "                        'b6589fc6ab0dc82cf12099d1c2d40ab994e8410c',\n",
    "                        \"incorrect value for sampleOHEDictManual[(0,'bear')]\")\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(0,'cat')],\n",
    "                        '356a192b7913b04c54574d18c28d46e6395428ab',\n",
    "                        \"incorrect value for sampleOHEDictManual[(0,'cat')]\")\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(0,'mouse')],\n",
    "                        'da4b9237bacccdf19c0760cab7aec4a8359010b0',\n",
    "                        \"incorrect value for sampleOHEDictManual[(0,'mouse')]\")\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(1,'black')],\n",
    "                        '77de68daecd823babbb58edb1c8e14d7106e83bb',\n",
    "                        \"incorrect value for sampleOHEDictManual[(1,'black')]\")\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(1,'tabby')],\n",
    "                        '1b6453892473a467d07372d45eb05abc2031647a',\n",
    "                        \"incorrect value for sampleOHEDictManual[(1,'tabby')]\")\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(2,'mouse')],\n",
    "                        'ac3478d69a3c81fa62e60f5c3696165a4e5e6ac4',\n",
    "                        \"incorrect value for sampleOHEDictManual[(2,'mouse')]\")\n",
    "Test.assertEqualsHashed(sampleOHEDictManual[(2,'salmon')],\n",
    "                        'c1dfd96eea8cc2b62785275bca38ac261256e278',\n",
    "                        \"incorrect value for sampleOHEDictManual[(2,'salmon')]\")\n",
    "Test.assertEquals(len(sampleOHEDictManual.keys()), 7,\n",
    "                  'incorrect number of keys in sampleOHEDictManual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1b) Sparse vectors **\n",
    "#### Data points can typically be represented with a small number of non-zero OHE features relative to the total number of features that occur in the dataset.  By leveraging this sparsity and using sparse vector representations of OHE data, we can reduce storage and computational burdens.  Below are a few sample vectors represented as dense numpy arrays.  Use [SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector) to represent them in a sparse fashion, and verify that both the sparse and dense representations yield the same results when computing [dot products](http://en.wikipedia.org/wiki/Dot_product) (we will later use MLlib to train classifiers via gradient descent, and MLlib will need to compute dot products between SparseVectors and dense parameter vectors).\n",
    "#### Use `SparseVector(size, *args)` to create a new sparse vector where size is the length of the vector and args is either a dictionary, a list of (index, value) pairs, or two separate arrays of indices and values (sorted by index).  You'll need to create a sparse vector representation of each dense vector `aDense` and `bDense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3\n",
      "7.3\n",
      "-0.5\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "aDense = np.array([0., 3., 0., 4.])\n",
    "#aSparse = <FILL IN>\n",
    "aSparse = SparseVector(4,[1,3],[3.,4.])\n",
    "\n",
    "bDense = np.array([0., 0., 0., 1.])\n",
    "#bSparse = <FILL IN>\n",
    "bSparse = SparseVector(4,[3],[1.])\n",
    "\n",
    "w = np.array([0.4, 3.1, -1.4, -.5])\n",
    "print aDense.dot(w)\n",
    "print aSparse.dot(w)\n",
    "print bDense.dot(w)\n",
    "print bSparse.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Sparse Vectors (1b)\n",
    "Test.assertTrue(isinstance(aSparse, SparseVector), 'aSparse needs to be an instance of SparseVector')\n",
    "Test.assertTrue(isinstance(bSparse, SparseVector), 'aSparse needs to be an instance of SparseVector')\n",
    "Test.assertTrue(aDense.dot(w) == aSparse.dot(w),\n",
    "                'dot product of aDense and w should equal dot product of aSparse and w')\n",
    "Test.assertTrue(bDense.dot(w) == bSparse.dot(w),\n",
    "                'dot product of bDense and w should equal dot product of bSparse and w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) OHE features as sparse vectors **\n",
    "#### Now let's see how we can represent the OHE features for points in our sample dataset.  Using the mapping defined by the OHE dictionary from Part (1a), manually define OHE features for the three sample data points using SparseVector format.  Any feature that occurs in a point should have the value 1.0.  For example, the `DenseVector` for a point with features 2 and 4 would be `[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reminder of the sample features\n",
    "# sampleOne = [(0, 'mouse'), (1, 'black')]\n",
    "# sampleTwo = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "# sampleThree =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#sampleOneOHEFeatManual = <FILL IN>\n",
    "sampleOneOHEFeatManual = SparseVector(7,[2,3],[1.,1.]) # [0.,0.,1.,1.,0.,0.,0.]\n",
    "sampleTwoOHEFeatManual = SparseVector(7,[1,4,5],[1.,1.,1.])  # [0.,1.,0.,0.,1.,1.,0.]\n",
    "sampleThreeOHEFeatManual = SparseVector(7,[0,3,6],[1.,1.,1.]) # [1.,0.,0.,1.,0.,0.,1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST OHE Features as sparse vectors (1c)\n",
    "Test.assertTrue(isinstance(sampleOneOHEFeatManual, SparseVector),\n",
    "                'sampleOneOHEFeatManual needs to be a SparseVector')\n",
    "Test.assertTrue(isinstance(sampleTwoOHEFeatManual, SparseVector),\n",
    "                'sampleTwoOHEFeatManual needs to be a SparseVector')\n",
    "Test.assertTrue(isinstance(sampleThreeOHEFeatManual, SparseVector),\n",
    "                'sampleThreeOHEFeatManual needs to be a SparseVector')\n",
    "Test.assertEqualsHashed(sampleOneOHEFeatManual,\n",
    "                        'ecc00223d141b7bd0913d52377cee2cf5783abd6',\n",
    "                        'incorrect value for sampleOneOHEFeatManual')\n",
    "Test.assertEqualsHashed(sampleTwoOHEFeatManual,\n",
    "                        '26b023f4109e3b8ab32241938e2e9b9e9d62720a',\n",
    "                        'incorrect value for sampleTwoOHEFeatManual')\n",
    "Test.assertEqualsHashed(sampleThreeOHEFeatManual,\n",
    "                        'c04134fd603ae115395b29dcabe9d0c66fbdc8a7',\n",
    "                        'incorrect value for sampleThreeOHEFeatManual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1d) Define a OHE function **\n",
    "#### Next we will use the OHE dictionary from Part (1a) to programatically generate OHE features from the original categorical data.  First write a function called `oneHotEncoding` that creates OHE feature vectors in `SparseVector` format.  Then use this function to create OHE features for the first sample data point and verify that the result matches the result from Part (1c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,[2,3],[1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    #<FILL IN>\n",
    "    \n",
    "    ##\n",
    "    ## THIS IS THE FIRST VERSION\n",
    "    ## It works but is very inefficient...\n",
    "    ##\n",
    "    #lenRaw = len(rawFeats)\n",
    "    #a = [0]*lenRaw\n",
    "    #b = [0]*lenRaw\n",
    "    #i = -1\n",
    "    #for x in rawFeats:\n",
    "    #    valAux = int(OHEDict[x])\n",
    "    #    i += 1\n",
    "    #    a[i] = int(valAux)\n",
    "    #    b[i] = 1.\n",
    "    #a = np.sort(a)\n",
    "    \n",
    "    ## Better version !!!\n",
    "    \n",
    "    a = [int(OHEDict[x]) for x in rawFeats]\n",
    "    a = np.sort(a)\n",
    "    b = [1. for x in rawFeats]\n",
    "    \n",
    "    return SparseVector(numOHEFeats,a,b)\n",
    "\n",
    "# Calculate the number of features in sampleOHEDictManual\n",
    "numSampleOHEFeats = len(sampleOHEDictManual)\n",
    "\n",
    "# Run oneHotEnoding on sampleOne\n",
    "sampleOneOHEFeat = oneHotEncoding(sampleOne, sampleOHEDictManual, len(sampleOHEDictManual))\n",
    "\n",
    "print sampleOneOHEFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Define an OHE Function (1d)\n",
    "Test.assertTrue(sampleOneOHEFeat == sampleOneOHEFeatManual,\n",
    "                'sampleOneOHEFeat should equal sampleOneOHEFeatManual')\n",
    "Test.assertEquals(sampleOneOHEFeat, SparseVector(7, [2,3], [1.0,1.0]),\n",
    "                  'incorrect value for sampleOneOHEFeat')\n",
    "Test.assertEquals(oneHotEncoding([(1, 'black'), (0, 'mouse')], sampleOHEDictManual,\n",
    "                                 numSampleOHEFeats), SparseVector(7, [2,3], [1.0,1.0]),\n",
    "                  'incorrect definition for oneHotEncoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1e) Apply OHE to a dataset **\n",
    "#### Finally, use the function from Part (1d) to create OHE features for all 3 data points in the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseVector(7, {2: 1.0, 3: 1.0}), SparseVector(7, {1: 1.0, 4: 1.0, 5: 1.0}), SparseVector(7, {0: 1.0, 3: 1.0, 6: 1.0})]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "sampleOHEData = sampleDataRDD.map(lambda x: oneHotEncoding(x, sampleOHEDictManual, len(sampleOHEDictManual)))\n",
    "print sampleOHEData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Apply OHE to a dataset (1e)\n",
    "sampleOHEDataValues = sampleOHEData.collect()\n",
    "Test.assertTrue(len(sampleOHEDataValues) == 3, 'sampleOHEData should have three elements')\n",
    "Test.assertEquals(sampleOHEDataValues[0], SparseVector(7, {2: 1.0, 3: 1.0}),\n",
    "                  'incorrect OHE for first sample')\n",
    "Test.assertEquals(sampleOHEDataValues[1], SparseVector(7, {1: 1.0, 4: 1.0, 5: 1.0}),\n",
    "                  'incorrect OHE for second sample')\n",
    "Test.assertEquals(sampleOHEDataValues[2], SparseVector(7, {0: 1.0, 3: 1.0, 6: 1.0}),\n",
    "                  'incorrect OHE for third sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Construct an OHE dictionary **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Pair RDD of `(featureID, category)` **\n",
    "#### To start, create an RDD of distinct `(featureID, category)` tuples. In our sample dataset, the 7 items in the resulting RDD are `(0, 'bear')`, `(0, 'cat')`, `(0, 'mouse')`, `(1, 'black')`, `(1, 'tabby')`, `(2, 'mouse')`, `(2, 'salmon')`. Notably `'black'` appears twice in the dataset but only contributes one item to the RDD: `(1, 'black')`, while `'mouse'` also appears twice and contributes two items: `(0, 'mouse')` and `(2, 'mouse')`.  Use [flatMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) and [distinct](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'mouse'), (0, 'cat'), (0, 'bear'), (2, 'salmon'), (1, 'tabby'), (1, 'black'), (0, 'mouse')]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "sampleDistinctFeats = (sampleDataRDD\n",
    "                       .flatMap(lambda x: x).distinct()) #.map(lambda x: (x[0],x[1])))\n",
    "print sampleDistinctFeats.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Pair RDD of (featureID, category) (2a)\n",
    "Test.assertEquals(sorted(sampleDistinctFeats.collect()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'incorrect value for sampleDistinctFeats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) OHE Dictionary from distinct features **\n",
    "#### Next, create an `RDD` of key-value tuples, where each `(featureID, category)` tuple in `sampleDistinctFeats` is a key and the values are distinct integers ranging from 0 to (number of keys - 1).  Then convert this `RDD` into a dictionary, which can be done using the `collectAsMap` action.  Note that there is no unique mapping from keys to values, as all we require is that each `(featureID, category)` key be mapped to a unique integer between 0 and the number of keys.  In this exercise, any valid mapping is acceptable.  Use [zipWithIndex](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) followed by [collectAsMap](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap).\n",
    "#### In our sample dataset, one valid list of key-value tuples is: `[((0, 'bear'), 0), ((2, 'salmon'), 1), ((1, 'tabby'), 2), ((2, 'mouse'), 3), ((0, 'mouse'), 4), ((0, 'cat'), 5), ((1, 'black'), 6)]`. The dictionary defined in Part (1a) illustrates another valid mapping between keys and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 'mouse'): 0, (0, 'cat'): 1, (0, 'bear'): 2, (2, 'salmon'): 3, (1, 'tabby'): 4, (1, 'black'): 5, (0, 'mouse'): 6}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "sampleOHEDict = (sampleDistinctFeats\n",
    "                           .zipWithIndex().collectAsMap())\n",
    "print sampleOHEDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST OHE Dictionary from distinct features (2b)\n",
    "Test.assertEquals(sorted(sampleOHEDict.keys()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'sampleOHEDict has unexpected keys')\n",
    "Test.assertEquals(sorted(sampleOHEDict.values()), range(7), 'sampleOHEDict has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) Automated creation of an OHE dictionary **\n",
    "#### Now use the code from Parts (2a) and (2b) to write a function that takes an input dataset and outputs an OHE dictionary.  Then use this function to create an OHE dictionary for the sample dataset, and verify that it matches the dictionary from Part (2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(2, 'mouse'): 0, (0, 'cat'): 1, (0, 'bear'): 2, (2, 'salmon'): 3, (1, 'tabby'): 4, (1, 'black'): 5, (0, 'mouse'): 6}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def createOneHotDict(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    return inputData.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "sampleOHEDictAuto = createOneHotDict(sampleDataRDD)\n",
    "print sampleOHEDictAuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Automated creation of an OHE dictionary (2c)\n",
    "Test.assertEquals(sorted(sampleOHEDictAuto.keys()),\n",
    "                  [(0, 'bear'), (0, 'cat'), (0, 'mouse'), (1, 'black'),\n",
    "                   (1, 'tabby'), (2, 'mouse'), (2, 'salmon')],\n",
    "                  'sampleOHEDictAuto has unexpected keys')\n",
    "Test.assertEquals(sorted(sampleOHEDictAuto.values()), range(7),\n",
    "                  'sampleOHEDictAuto has unexpected values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Parse CTR data and generate OHE features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we can proceed, you'll first need to obtain the data from Criteo.  If you have already completed this step in the setup lab, just run the cells below and the data will be loaded into the `rawData` variable.\n",
    "#### Below is Criteo's data sharing agreement.  After you accept the agreement, you can obtain the download URL by right-clicking on the \"Download Sample\" button and clicking \"Copy link address\" or \"Copy Link Location\", depending on your browser.  Paste the URL into the `# TODO` cell below.  The file is 8.4 MB compressed.  The script below will download the file to the virtual machine (VM) and then extract the data.\n",
    "#### If running the cell below does not render a webpage, open the [Criteo agreement](http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/) in a separate browser tab.  After you accept the agreement, you can obtain the download URL by right-clicking on the \"Download Sample\" button and clicking \"Copy link address\" or \"Copy Link Location\", depending on your browser.  Paste the URL into the `# TODO` cell below.\n",
    "#### Note that the download could take a few minutes, depending upon your connection speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"350\"\n",
       "            src=\"http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xb0ec0c4c>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this code to view Criteo's agreement\n",
    "from IPython.lib.display import IFrame\n",
    "\n",
    "IFrame(\"http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/\",\n",
    "       600, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is already available. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Just replace <FILL IN> with the url for dac_sample.tar.gz\n",
    "import glob\n",
    "import os.path\n",
    "import tarfile\n",
    "import urllib\n",
    "import urlparse\n",
    "\n",
    "# Paste url, url should end with: dac_sample.tar.gz\n",
    "url = 'http://labs.criteo.com/wp-content/uploads/2015/04/dac_sample.tar.gz'\n",
    "\n",
    "url = url.strip()\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('cs190', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "inputDir = os.path.split(fileName)[0]\n",
    "\n",
    "def extractTar(check = False):\n",
    "    # Find the zipped archive and extract the dataset\n",
    "    tars = glob.glob('dac_sample*.tar.gz*')\n",
    "    if check and len(tars) == 0:\n",
    "      return False\n",
    "\n",
    "    if len(tars) > 0:\n",
    "        try:\n",
    "            tarFile = tarfile.open(tars[0])\n",
    "        except tarfile.ReadError:\n",
    "            if not check:\n",
    "                print 'Unable to open tar.gz file.  Check your URL.'\n",
    "            return False\n",
    "\n",
    "        tarFile.extract('dac_sample.txt', path=inputDir)\n",
    "        print 'Successfully extracted: dac_sample.txt'\n",
    "        return True\n",
    "    else:\n",
    "        print 'You need to retry the download with the correct url.'\n",
    "        print ('Alternatively, you can upload the dac_sample.tar.gz file to your Jupyter root ' +\n",
    "              'directory')\n",
    "        return False\n",
    "\n",
    "\n",
    "if os.path.isfile(fileName):\n",
    "    print 'File is already available. Nothing to do.'\n",
    "elif extractTar(check = True):\n",
    "    print 'tar.gz file was already available.'\n",
    "elif not url.endswith('dac_sample.tar.gz'):\n",
    "    print 'Check your download url.  Are you downloading the Sample dataset?'\n",
    "else:\n",
    "    # Download the file and store it in the same directory as this notebook\n",
    "    try:\n",
    "        urllib.urlretrieve(url, os.path.basename(urlparse.urlsplit(url).path))\n",
    "    except IOError:\n",
    "        print 'Unable to download and store: {0}'.format(url)\n",
    "\n",
    "    extractTar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('cs190', 'dac_sample.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "if os.path.isfile(fileName):\n",
    "    rawData = (sc\n",
    "               .textFile(fileName, 2)\n",
    "               .map(lambda x: x.replace('\\t', ',')))  # work with either ',' or '\\t' separated data\n",
    "    print rawData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3a) Loading and splitting the data **\n",
    "#### We are now ready to start working with the actual CTR data, and our first task involves splitting it into training, validation, and test sets.  Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) with the specified weights and seed to create RDDs storing each of these datasets, and then [cache](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache) each of these RDDs, as we will be accessing them multiple times in the remainder of this lab. Finally, compute the size of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79911 10075 10014 100000\n",
      "[u'0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights,seed=seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "print rawData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Loading and splitting the data (3a)\n",
    "Test.assertTrue(all([rawTrainData.is_cached, rawValidationData.is_cached, rawTestData.is_cached]),\n",
    "                'you must cache the split data')\n",
    "Test.assertEquals(nTrain, 79911, 'incorrect value for nTrain')\n",
    "Test.assertEquals(nVal, 10075, 'incorrect value for nVal')\n",
    "Test.assertEquals(nTest, 10014, 'incorrect value for nTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Extract features **\n",
    "#### We will now parse the raw training data to create an RDD that we can subsequently use to create an OHE dictionary. Note from the `take()` command in Part (3a) that each raw data point is a string containing several fields separated by some delimiter.  For now, we will ignore the first field (which is the 0-1 label), and parse the remaining fields (or raw features).  To do this, complete the implemention of the `parsePoint` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    #aux = point[0].split(',')\n",
    "    #return list(enumerate(aux[1:]))\n",
    "    \n",
    "    #aux = point.split(',')\n",
    "    aux = point.split(',')\n",
    "    return list(enumerate(aux[1:]))\n",
    "    \n",
    "\n",
    "parsedTrainFeat = rawTrainData.map(parsePoint)\n",
    "\n",
    "numCategories = (parsedTrainFeat\n",
    "                 .flatMap(lambda x: x)\n",
    "                 .distinct()\n",
    "                 .map(lambda x: (x[0], 1))\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "                 .sortByKey()\n",
    "                 .collect())\n",
    "\n",
    "\n",
    "print numCategories[2][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "# TEST Extract features (3b)\n",
    "Test.assertEquals(numCategories[2][1], 855, 'incorrect implementation of parsePoint')\n",
    "Test.assertEquals(numCategories[32][1], 4, 'incorrec